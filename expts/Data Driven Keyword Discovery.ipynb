{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load excel from /Users/qli/Documents/trusty64/reli/reli_30h剩余数据.csv\n",
      "Fin!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install xlrd\n",
    "import pandas as pd\n",
    "\n",
    "def load_reli(filename):\n",
    "    try:\n",
    "        df = pd.read_excel(filename)\n",
    "        lines = []\n",
    "        for i in range(0, len(df)):\n",
    "            if (df.iloc[i, 7] > 0):\n",
    "                lines.append(df.iloc[i, 1])\n",
    "        return lines\n",
    "    except:\n",
    "        print('Failed to load excel from ' + filename)\n",
    "        return []\n",
    "\n",
    "lines = load_reli('/Users/qli/Documents/trusty64/reli/reli_30h剩余数据.csv')\n",
    "with open('/tmp/test', 'wt') as fout:\n",
    "    fout.write('\\n'.join(lines).encode('utf-8'))\n",
    "\n",
    "print('Fin!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load reli data from /Users/qli/Documents/trusty64/reli/热力23H.csv\n",
      "load reli data from /Users/qli/Documents/trusty64/reli/reli_30h剩余数据.csv\n",
      "Failed to load excel from /Users/qli/Documents/trusty64/reli/reli_30h剩余数据.csv\n",
      "/Users/qli/Documents/trusty64/reli/reli_30h剩余数据.csv contains no reli data\n",
      "load reli data from /Users/qli/Documents/trusty64/reli/reli_02.csv\n",
      "load reli data from /Users/qli/Documents/trusty64/reli/reli_03.csv\n",
      "load reli data from /Users/qli/Documents/trusty64/reli/reli_8k_100h_choose_v2_1.csv\n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def load_reli_glob(glob_files):\n",
    "    lines = []\n",
    "    for filename in glob.glob(glob_files):\n",
    "        print('load reli data from ' + filename)\n",
    "        new_lines = load_reli(filename)\n",
    "        if lines is not None and len(new_lines) > 0:\n",
    "            lines = lines + new_lines\n",
    "        else:\n",
    "            print(filename + ' contains no reli data')\n",
    "    return lines\n",
    "\n",
    "reli_lines = load_reli_glob('/Users/qli/Documents/trusty64/reli/*.csv')\n",
    "with open('/Users/qli/Documents/trusty64/reli/reli_all.txt', 'wt') as fout:\n",
    "    fout.write('\\n'.join(reli_lines).encode('utf-8'))\n",
    "print('Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin\n"
     ]
    }
   ],
   "source": [
    "# install jieba\n",
    "import jieba\n",
    "\n",
    "'''\n",
    "按照字符来分割\n",
    "'''\n",
    "def seg_by_char(line):\n",
    "    line = line.decode('utf8')\n",
    "    seg_list = []\n",
    "    for c in line:\n",
    "        seg_list.append(c)\n",
    "    return seg_list\n",
    "\n",
    "def process_reli(filename, segmenter):\n",
    "    with open(filename, 'rt') as fin:\n",
    "        lines = [re.sub(r'<.*?>|\\[.*?\\]', '', line.strip(), flags=re.IGNORECASE) for line in fin]\n",
    "    segmented_lines = []\n",
    "    if (segmenter == 'jieba'):\n",
    "        for line in lines:\n",
    "            seg_list = jieba.cut(line, cut_all=False)  # 精确模式\n",
    "            segmented_lines.append(' '.join(seg_list))\n",
    "    elif (segmenter == 'by_char'):\n",
    "        for line in lines:\n",
    "            seg_list = seg_by_char(line)\n",
    "            segmented_lines.append(' '.join(seg_list))\n",
    "    else:\n",
    "        print('unrecognized segmenter ' + segmenter)\n",
    "        system.exit(1)\n",
    "    return segmented_lines\n",
    "\n",
    "corpus_jieba = process_reli('/Users/qli/Documents/trusty64/reli/reli_all.txt', 'jieba')\n",
    "corpus_by_char = process_reli('/Users/qli/Documents/trusty64/reli/reli_all.txt', 'by_char')\n",
    "\n",
    "with open('/Users/qli/Documents/trusty64/reli/reli_all.txt.jieba', 'wt') as fout:\n",
    "    fout.write('\\n'.join(corpus_jieba).encode('utf-8'))\n",
    "    \n",
    "with open('/Users/qli/Documents/trusty64/reli/reli_all.txt.by_char', 'wt') as fout:\n",
    "    fout.write('\\n'.join(corpus_by_char).encode('utf-8'))\n",
    "\n",
    "print('Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 1-gram\n",
      "Process 2-gram\n",
      "Process 3-gram\n",
      "Process 4-gram\n",
      "-----------ngram new lexicon-------------\n",
      "高兴 为您服务\n",
      "蓝调 沙龙\n",
      "赛 洛城\n",
      "珠江 帝景\n",
      "-----------ngram new lexicon-------------\n",
      "\n",
      "-----------ngram new lexicon-------------\n",
      "请问 有 什么 可以\n",
      "A   B\n",
      "并 进行 供热 调节\n",
      "安化 北 里 二号\n",
      "使用 呼叫 保持 服务\n",
      "正在 使用 呼叫 保持\n"
     ]
    }
   ],
   "source": [
    "# DATA-DRIVEN LEXICON EXPANSION FOR MANDARIN BROADCAST NEWS AND CONVERSATION SPEECH RECOGNITION\n",
    "import math\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "class Ngram:\n",
    "    word_list = []\n",
    "    count = 0\n",
    "    \n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "        self.count = 1\n",
    "        \n",
    "    def get_n(self):\n",
    "        return len(self.word_list)\n",
    "    \n",
    "    def add_count(self):\n",
    "        self.count = self.count + 1\n",
    "        \n",
    "    def get_count(self):\n",
    "        return self.count\n",
    "    \n",
    "    def get_nth_gram(self, n):\n",
    "        return self.word_list[n]\n",
    "    \n",
    "    def get_word_list(self):\n",
    "        return self.word_list\n",
    "    \n",
    "    def to_string(self):\n",
    "        return u' '.join(self.word_list)\n",
    "    \n",
    "def count_ngram(corpus, n):\n",
    "    lines = corpus\n",
    "    ngram_counts = {}\n",
    "    for line in lines:\n",
    "        seg_list = line.split(u' ')\n",
    "        for i in range(0, len(seg_list)):\n",
    "            if (i + n > len(seg_list)):\n",
    "                continue\n",
    "            ngram = Ngram(seg_list[i : i + n])\n",
    "            lexicon = ngram.to_string()\n",
    "            # print('lexicon ' + lexicon + ' len ' + str(len(lexicon)))\n",
    "            if lexicon in ngram_counts:\n",
    "                ngram_counts[lexicon].add_count()\n",
    "            else:\n",
    "                ngram_counts[lexicon] = ngram\n",
    "    return ngram_counts\n",
    "\n",
    "def find_ngram(ngram_counts, key):\n",
    "    if (key in ngram_counts):\n",
    "        return ngram_counts[key]\n",
    "    else:\n",
    "        print('key ' + key + ' doesnt exist in ngram dict')\n",
    "        traceback.print_stack()\n",
    "        sys.exit(1)\n",
    "\n",
    "def compute_LM2_measure(ngram_counts, ngram_wij, ngram_wi, ngram_wj):\n",
    "    N_wi_wj = ngram_wij.get_count()\n",
    "    N_wi = ngram_wi.get_count()\n",
    "    N_wj = ngram_wj.get_count()\n",
    "    return float(N_wi_wj) / math.sqrt(N_wi * N_wj)\n",
    "        \n",
    "def compute_LM3_measure(ngram_counts, ngram):\n",
    "    if (ngram.get_n() != 3):\n",
    "        sys.exit(1)\n",
    "    Ngram_wi_wj = find_ngram(ngram_counts, Ngram(ngram.get_word_list()[0 : 2]).to_string())\n",
    "    Ngram_wk = find_ngram(ngram_counts, Ngram(ngram.get_word_list()[2 : 3]).to_string())\n",
    "    Ngram_wj_wk = find_ngram(ngram_counts, Ngram(ngram.get_word_list()[1 : 3]).to_string())\n",
    "    Ngram_wi = find_ngram(ngram_counts, Ngram(ngram.get_word_list()[0 : 1]).to_string())\n",
    "    LM2_measure_1 = compute_LM2_measure(ngram_counts, ngram, Ngram_wi_wj, Ngram_wk)\n",
    "    LM2_measure_2 = compute_LM2_measure(ngram_counts, ngram, Ngram_wi, Ngram_wj_wk)\n",
    "    return math.sqrt(LM2_measure_1 * LM2_measure_2)\n",
    "\n",
    "def compute_LM4_measure(ngram_counts, ngram):\n",
    "    if (ngram.get_n() != 4):\n",
    "        sys.exit(1)\n",
    "    Ngram_wi_wj = ngram_counts[Ngram(ngram.get_word_list()[0 : 2]).to_string()]\n",
    "    Ngram_wk_wl = ngram_counts[Ngram(ngram.get_word_list()[2 : 4]).to_string()]\n",
    "    return compute_LM2_measure(ngram_counts, ngram, Ngram_wi_wj, Ngram_wk_wl)\n",
    "\n",
    "def compute_LM(ngram_counts, new_ngram):\n",
    "    # print('compute_LM 1:' + lexicon + ' len:' + str(len(lexicon_segs)))\n",
    "    n = new_ngram.get_n()\n",
    "    if (n == 2):\n",
    "        Ngram_wi = find_ngram(ngram_counts, Ngram(new_ngram.get_word_list()[0 : 1]).to_string())\n",
    "        Ngram_wj = find_ngram(ngram_counts, Ngram(new_ngram.get_word_list()[1 : 2]).to_string())\n",
    "        return compute_LM2_measure(ngram_counts, new_ngram, Ngram_wi, Ngram_wj)\n",
    "    if (n == 3):\n",
    "        return compute_LM3_measure(ngram_counts, new_ngram)\n",
    "    if (n == 4):\n",
    "        return compute_LM4_measure(ngram_counts, new_ngram)\n",
    "    else:\n",
    "        print(u'ERROR: unsupported n ' + str(n))\n",
    "        sys.exit(0)\n",
    "        \n",
    "# corpus: a list of queries\n",
    "def data_driven_lexicon_discovery(corpus, M, theta, beta):\n",
    "    N = 4\n",
    "    new_ngrams = {}  # key = n in ngram, value = new lexicon list\n",
    "    all_ngram_counts = {}\n",
    "    for i in range(1, N + 1):\n",
    "        print('Process ' + str(i) + \"-gram\")\n",
    "        ngram_counts = count_ngram(corpus, i)\n",
    "        \n",
    "        sorted_ngram_counts = sorted(ngram_counts.items(), key=lambda x: x[1].get_count(), reverse=True)  # sort n-grams by counts\n",
    "\n",
    "        all_ngram_counts = dict(all_ngram_counts, **ngram_counts)  # dump n-gram counts\n",
    "        if (i > 1):\n",
    "            new_ngrams[i] = []\n",
    "            for j in range(0, M):  # foreach element Wn of the topM n-grams do\n",
    "                new_ngram = sorted_ngram_counts[j][1]\n",
    "                score = compute_LM(all_ngram_counts, new_ngram)  # compute the LMn(·) measure\n",
    "                # print('score = ' + str(score))\n",
    "                if (score > theta):\n",
    "                    new_ngrams[i].append(new_ngram)  # put new lexicon at nth list which indicate a lexicon of ngram\n",
    "    for i in range(N - 1, 1, -1):  # for n ← (N − 1) to 2 do\n",
    "        for w_n in new_ngrams[i]:  # foreach element Wn of the saved n-grams do\n",
    "            for w_m in new_ngrams[i + 1]:  # foreach element Wm of the saved higher-order\n",
    "                if (w_n.to_string() in w_m.to_string()):  # if Wm contains Wn then\n",
    "                    r = float(all_ngram_counts[w_n].get_count()) / float(all_ngram_counts[w_m].get_count())  # compute counts ratio r of Wn and Wm\n",
    "                    # print('ratio = ' + str(r))\n",
    "                    if r < beta:  # if r < β then\n",
    "                        print('remove ' + w_n)\n",
    "                        new_ngrams[i].remove(w_n)  # remove Wn from saved n-grams\n",
    "    for ngrams in new_ngrams.values():\n",
    "        print('-----------ngram new lexicon-------------')\n",
    "        print('\\n'.join(ngram.to_string() for ngram in ngrams))\n",
    "            \n",
    "\n",
    "with open('reli_all.txt.jjieba', 'rt') as fin:\n",
    "    corpus = [line.decode('utf8').strip() for line in fin]\n",
    "\n",
    "data_driven_lexicon_discovery(corpus, M = 5000, theta = 0.9, beta = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
